# AI Assistant for NASA's Worldview - Luma

**Luma** is an AI-powered Earth observation assistant built with **LangGraph**, **FastAPI**, and **NASA Worldview/GIBS APIs**.  
It lets users request imagery layers (e.g., MODIS, VIIRS, fires, true color) and automatically generates shareable NASA Worldview links or static satellite snapshots â€” all through a conversational agent interface.

---

## ğŸš€ Features

- ğŸ¤– **LangGraph Agent** â€” interprets natural-language prompts (â€œshow me wildfires in California yesterdayâ€).
- ğŸ›°ï¸ **NASA Worldview Integration** â€” builds interactive map links or retrieves static imagery via Worldview Snapshots.
- ğŸ“¡ **Layer Metadata Lookup** â€” fetches layer descriptions via NASAâ€™s CMR Visualizations API or `wv.json`.
- ğŸŒ **FastAPI Backend** â€” clean JSON API + streaming (SSE / WebSocket).
- ğŸ”Œ **Frontend Ready** â€” CORS-enabled; connect via Fetch, SSE, or WebSocket.
- ğŸ§  **Modular Graph** â€” easily extend agent nodes for RAG, reasoning, or analysis.
- â˜ï¸ **Deploy Anywhere** â€” runs locally via Uvicorn or in production on AWS (Lambda, ECS, App Runner, or EC2).

---

## Setting up the project
  1. Install python 3.12
  2. Install poetry
  3. Use `poetry install`

## Running the project locally
  Run `poetry run uvicorn src.nasaworldviewassistant.main:app --host 0.0.0.0 --port 8000 --reload`
  You can run `curl -X POST "http://0.0.0.0:8000/api/agent"      -H "Content-Type: application/json"      -d '{"input": "Hello agent!", "thread_id": "thread-001"}'`
  or go to `http://localhost:8000/docs#/` and run the api command from there
  
