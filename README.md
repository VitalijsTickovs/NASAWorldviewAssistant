# AI Assistant for NASA's Worldview - Luma

**Luma** is an AI-powered Earth observation assistant built with **LangGraph**, **FastAPI**, and **NASA Worldview/GIBS APIs**.  
It lets users request imagery layers (e.g., MODIS, VIIRS, fires, true color) and automatically generates shareable NASA Worldview links or static satellite snapshots â€” all through a conversational agent interface.

---

## ðŸš€ Features

- ðŸ¤– **LangGraph Agent** â€” interprets natural-language prompts (â€œshow me wildfires in California yesterdayâ€).
- ðŸ›°ï¸ **NASA Worldview Integration** â€” builds interactive map links or retrieves static imagery via Worldview Snapshots.
- ðŸ“¡ **Layer Metadata Lookup** â€” fetches layer descriptions via NASAâ€™s CMR Visualizations API or `wv.json`.
- ðŸŒ **FastAPI Backend** â€” clean JSON API + streaming (SSE / WebSocket).
- ðŸ”Œ **Frontend Ready** â€” CORS-enabled; connect via Fetch, SSE, or WebSocket.
- ðŸ§  **Modular Graph** â€” easily extend agent nodes for RAG, reasoning, or analysis.
- â˜ï¸ **Deploy Anywhere** â€” runs locally via Uvicorn or in production on AWS (Lambda, ECS, App Runner, or EC2).

---

## ðŸ§© Architecture Overview

```mermaid
flowchart LR
  U[User / Frontend] -->|JSON / SSE / WS| A[FastAPI API]
  A -->|invoke| G[LangGraph Agent]
  G -->|build request| N[NASA APIs]
  N -->|Worldview Snapshots / GIBS / CMR / EONET| G
  A -->|return| U
  subgraph "Optional Infra"
    R[(Redis / Cache)]
    L[(Logging / CloudWatch)]
  end
  G <-->|checkpoint| R
  A --> L

# NASA's Worldview AI Assistant - Luma

**Luma** is an AI-powered Earth observation assistant built with **LangGraph**, **FastAPI**, and **NASA Worldview/GIBS APIs**.  
It lets users request imagery layers (e.g., MODIS, VIIRS, fires, true color) and automatically generates shareable NASA Worldview links or static satellite snapshots â€” all through a conversational agent interface.

---

## ðŸš€ Features

- ðŸ¤– **LangGraph Agent** â€” interprets natural-language prompts (â€œshow me wildfires in California yesterdayâ€).
- ðŸ›°ï¸ **NASA Worldview Integration** â€” builds interactive map links or retrieves static imagery via Worldview Snapshots.
- ðŸ“¡ **Layer Metadata Lookup** â€” fetches layer descriptions via NASAâ€™s CMR Visualizations API or `wv.json`.
- ðŸŒ **FastAPI Backend** â€” clean JSON API + streaming (SSE / WebSocket).
- ðŸ”Œ **Frontend Ready** â€” CORS-enabled; connect via Fetch, SSE, or WebSocket.
- ðŸ§  **Modular Graph** â€” easily extend agent nodes for RAG, reasoning, or analysis.
- â˜ï¸ **Deploy Anywhere** â€” runs locally via Uvicorn or in production on AWS (Lambda, ECS, App Runner, or EC2).

---

## ðŸ§© Architecture Overview

```mermaid
flowchart LR
  U[User / Frontend] -->|JSON / SSE / WS| A[FastAPI API]
  A -->|invoke| G[LangGraph Agent]
  G -->|build request| N[NASA APIs]
  N -->|Worldview Snapshots / GIBS / CMR / EONET| G
  A -->|return| U
  subgraph "Optional Infra"
    R[(Redis / Cache)]
    L[(Logging / CloudWatch)]
  end
  G <-->|checkpoint| R
  A --> L

## Setting up the project
  1. Install python 3.12
  2. Install poetry
  3. Use `poetry install`

## Running the project locally
  Run `poetry run uvicorn src.nasaworldviewassistant.main:app --host 0.0.0.0 --port 8000 --reload`
  You can run `curl -X POST "http://0.0.0.0:8000/api/agent"      -H "Content-Type: application/json"      -d '{"input": "Hello agent!", "thread_id": "thread-001"}'`
  or go to `http://localhost:8000/docs#/` and run the api command from there
  